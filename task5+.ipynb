{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【Task5(2天)】PyTorch实现L1，L2正则化以及Dropout(给代码截图参考)\n",
    "- 了解知道Dropout原理\n",
    "\n",
    "神经网络中的正则化方法用于防止过拟合，除了机器学习中，常用的L1,L2正则化以外，Dropout神经元随机失活也是一种过拟合的解决方法。\n",
    "在训练过程中，以概率p将神经元权重置为0，即随机失活，测试时，所有神经元均呈激活态，但权重需要乘以(1-p)，保证训练测试时，各自权重期望相同。对于两层三个神经元的结构，就有9种(3*3 )网络结构，相当于网络的平均集成，具有更高的泛化能力，有效防止过拟合。\n",
    "- 用代码实现正则化(L1、L2、Dropout）\n",
    "\n",
    "L1,L2正则化通常式对模型的参数进行约束，通过加入到损失函数整体，利用反向传播进行优化。L2约束参数量级，L1还能使参数更加稀疏，可以进行特征选择。L2正则化训练的收敛速度一般比L1正则快。二者的组合有Elastic网络正则化方法，以及最大范数约束，将L1或L2范数的取值范围不大于某个常数。\n",
    "\n",
    "- Dropout的numpy实现\n",
    "- PyTorch中实现dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# L1,L2,elastic-net正则化\n",
    "def l1_normal(x):\n",
    "    return torch.abs(x).sum()\n",
    "def l2_normal(x):\n",
    "    return torch.pow(x,2).sum()\n",
    "def elastic_net(x,a,b):\n",
    "    return l1_normal(x)*a+l2_normal(x)*b\n",
    "#分类问题中，在原来的交叉熵损失函数中加入对于模型参数的限制\n",
    "l1 = lambda1*l1_normal(layer1)\n",
    "l2 = lambda2*l2_normal(layer2)\n",
    "el = lambda2*elastic_net(layer3,0.5,0.5)\n",
    "loss = criterion(y_pred,y) + l1 + l2 + el\n",
    "# 可能由于数据选择的较简单，加了正则化之后，准确率下降了10%以上，只有80%,导致欠拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pytorch中dropout的实现,在非线性层之后加了随机失活层\n",
    "F.dropout(input, p=0.5, training=False, inplace=False)\n",
    "# 任务四改进：\n",
    "class MyClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyClassifier,self).__init__()\n",
    "        self.fc1 = nn.Linear(2,4)\n",
    "        self.fc2 = nn.Linear(4,6)\n",
    "        self.fc3 = nn.Linear(6,3)\n",
    "        self.fc4 = nn.Linear(3,2)\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        a1 = F.tanh(x)\n",
    "        x = F.dropout(a1,p=0.5)\n",
    "        x = self.fc2(a1)\n",
    "        a2 = F.tanh(x)\n",
    "        x = F.dropout(a2,p=0.5)\n",
    "        x = self.fc3(a2)\n",
    "        a3 = F.relu(x)\n",
    "        x = F.dropout(a3,p=0.5)\n",
    "        x = self.fc4(x)\n",
    "        return a1,a2,a3,x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dropout numpy实现\n",
    "def dropout(x, p): \n",
    "    # 测试时需要乘的1-p\n",
    "    retain_prob = 1 - p  \n",
    "    # 利用binomial函数，生成与x一样的维数向量。\n",
    "    # 神经元x保留的概率为p，n表示每个神经元参与随机实验的次数，通常为1,。\n",
    "    # size是神经元总数。  \n",
    "    sample=np.random.binomial(n=1,p=retain_prob,size=x.shape)\n",
    "    # 生成一个0、1分布的向量，0表示该神经元被丢弃 \n",
    "    # print sample  \n",
    "    x *=sample\n",
    "    # 规范化  \n",
    "    x /= retain_prob  \n",
    "    return x  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
